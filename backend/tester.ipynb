{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68838b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = [\n",
    "    r'''Matias Cattaneo\n",
    "Position\n",
    "Professor\n",
    "Website\n",
    "Matias Cattaneo's Site\n",
    "Office Phone\n",
    "(609) 258-8825\n",
    "Email\n",
    "cattaneo@princeton.edu\n",
    "Office\n",
    "230 - Sherrerd Hall\n",
    "Bio/Description\n",
    "Research Interests: Econometrics, statistics, machine learning, data science, causal inference, program evaluation, quantitative methods in the social, behavioral and biomedical sciences.''',\n",
    "    r'''Jianqing Fan\n",
    "Position\n",
    "Frederick L. Moore Professor in Finance\n",
    "Website\n",
    "Jianqing Fan's Site\n",
    "Office Phone\n",
    "(609) 258-7924\n",
    "Email\n",
    "jqfan@princeton.edu\n",
    "Office\n",
    "205 - Sherrerd Hall\n",
    "Bio/Description\n",
    "Research Interests: High-dimensional statistics, Machine Learning, financial econometrics, computational biology, biostatistics, graphical and network modeling, portfolio theory, high-frequency finance, time series.''',\n",
    "    r'''Jason Klusowski\n",
    "Position\n",
    "Assistant Professor\n",
    "Website\n",
    "Jason Klusowski's Site\n",
    "Office Phone\n",
    "(609) 258-5305\n",
    "Email\n",
    "jason.klusowski@princeton.edu\n",
    "Office\n",
    "327 - Sherrerd Hall\n",
    "Bio/Description\n",
    "Research Interests: Data science, statistical learning, deep learning, decision tree learning; high-dimensional statistics, information theory, statistical physics, network modeling'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a772eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import anthropic\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "novita_client = OpenAI(\n",
    "    api_key=os.getenv(\"novita_api_key\"),\n",
    "    base_url=\"https://api.novita.ai/openai\"\n",
    ")\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.getenv(\"openai_api_key\")\n",
    ")\n",
    "\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"claude_api_key\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9851ed38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. I'm doing well, thanks for asking. I'm a large language model, so I don't have feelings or emotions like humans do, but I'm always happy to help with any questions or tasks you may have. How about you? How's your day going so far? Is there anything I can help you with or would you like to chat?\n",
      "CompletionUsage(completion_tokens=74, prompt_tokens=47, total_tokens=121, completion_tokens_details=None, prompt_tokens_details=None)\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"novita_api_key\"),\n",
    "    base_url=\"https://api.novita.ai/openai\"\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, how are you?\"}\n",
    "    ],\n",
    "    max_tokens=120000,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(response.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1de16804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task prompt created and formatted with profiles\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Define the task prompt for analyzing professor profiles\n",
    "task_prompt = \"\"\"\n",
    "You are an expert academic researcher. Please analyze the following professor profiles and provide:\n",
    "\n",
    "1. A summary of each professor's research focus\n",
    "2. Potential collaboration opportunities between them\n",
    "3. Emerging research trends in their fields\n",
    "4. Recommendations for interdisciplinary research projects\n",
    "\n",
    "Here are the professor profiles:\n",
    "\n",
    "{profiles}\n",
    "\n",
    "Please provide a comprehensive analysis that would be valuable for academic planning and research strategy.\n",
    "\"\"\"\n",
    "\n",
    "# Format the prompt with the actual profiles\n",
    "formatted_prompt = task_prompt.format(profiles=\"\\n\\n\".join(profiles))\n",
    "print(\"Task prompt created and formatted with profiles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5af9b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with OpenAI GPT-4...\n",
      "OpenAI Response:\n",
      "==================================================\n",
      "1. Summary of each professor's research focus:\n",
      "\n",
      "Matias Cattaneo's research primarily revolves around econometrics, statistics, machine learning, data science, causal inference, program evaluation, and quantitative methods in the social, behavioral and biomedical sciences. This suggests a comprehensive approach to data analysis and modeling, with a particular focus on developing methods that can help understand the impact of various programs or interventions.\n",
      "\n",
      "Jianqing Fan's research interests lie in high-dimensional statistics, machine learning, financial econometrics, computational biology, biostatistics, graphical and network modeling, portfolio theory, high-frequency finance, and time series. This indicates a strong focus on applying statistical tools and techniques to finance and biology, developing models that can help understand complex systems and make predictions.\n",
      "\n",
      "Jason Klusowski's research interests include data science, statistical learning, deep learning, decision tree learning, high-dimensional statistics, information theory, statistical physics, and network modeling. His work seems to be at the intersection of statistics and computer science, developing and applying machine learning algorithms to solve complex problems.\n",
      "\n",
      "2. Potential collaboration opportunities between them:\n",
      "\n",
      "Given their shared interests in machine learning, high-dimensional statistics, and data science, there are several potential areas for collaboration. They could work together on developing new statistical methods for dealing with high-dimensional data, or on applying machine learning techniques to econometrics or finance. They could also collaborate on creating more effective models for understanding complex social, behavioral, biomedical, or financial systems.\n",
      "\n",
      "3. Emerging research trends in their fields:\n",
      "\n",
      "In the fields of econometrics, statistics, and machine learning, emerging trends include the increasing use of machine learning techniques for econometric modeling, the development of new methods for dealing with high-dimensional data, and the application of these methods to more diverse fields such as social sciences, finance, and biology. There is also a growing interest in causal inference, and in understanding not just correlations but also the underlying mechanisms that drive observed patterns.\n",
      "\n",
      "4. Recommendations for interdisciplinary research projects:\n",
      "\n",
      "One potential interdisciplinary project could involve developing new machine learning techniques for financial econometrics, combining the expertise of Cattaneo in econometrics and data science, Fan in financial econometrics and machine learning, and Klusowski in statistical learning and deep learning. Another project could focus on applying high-dimensional statistics to understand complex biological systems, leveraging the expertise of Fan in computational biology and high-dimensional statistics, and of Klusowski in high-dimensional statistics and network modeling. Yet another project could involve using machine learning and high-dimensional statistics to evaluate the impact of social or health programs, combining the expertise of Cattaneo in econometrics, program evaluation and causal inference, and of Klusowski in data science and machine learning.\n",
      "==================================================\n",
      "Token usage: CompletionUsage(completion_tokens=541, prompt_tokens=356, total_tokens=897, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Test the prompt with OpenAI\n",
    "print(\"Testing with OpenAI GPT-4...\")\n",
    "\n",
    "openai_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "    ],\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "openai_output = openai_response.choices[0].message.content\n",
    "openai_usage = openai_response.usage\n",
    "\n",
    "print(\"OpenAI Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(openai_output)\n",
    "print(\"=\" * 50)\n",
    "print(f\"Token usage: {openai_usage}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9280c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with Llama 3.3 70B via Novita...\n",
      "Llama Response:\n",
      "==================================================\n",
      "**Summary of Each Professor's Research Focus**\n",
      "\n",
      "1. **Matias Cattaneo**: Professor Cattaneo's research focus is on econometrics, statistics, machine learning, and data science, with applications in social, behavioral, and biomedical sciences. His work on causal inference, program evaluation, and quantitative methods suggests a strong emphasis on developing and applying statistical techniques to understand complex phenomena.\n",
      "2. **Jianqing Fan**: Professor Fan's research interests span high-dimensional statistics, machine learning, financial econometrics, and computational biology. His work in portfolio theory, high-frequency finance, and time series analysis indicates a strong background in finance and economics, with a focus on developing statistical and computational methods for analyzing complex financial data.\n",
      "3. **Jason Klusowski**: Professor Klusowski's research focus is on data science, statistical learning, and deep learning, with a particular emphasis on high-dimensional statistics, information theory, and network modeling. His work in decision tree learning and statistical physics suggests a strong interest in developing and applying machine learning techniques to understand complex systems.\n",
      "\n",
      "**Potential Collaboration Opportunities**\n",
      "\n",
      "1. **Cattaneo and Fan**: Collaboration on projects related to financial econometrics, causal inference, and program evaluation could leverage their combined expertise in econometrics, statistics, and machine learning.\n",
      "2. **Fan and Klusowski**: Joint research on high-dimensional statistics, machine learning, and network modeling could lead to innovative applications in finance, economics, and computational biology.\n",
      "3. **Cattaneo and Klusowski**: Collaboration on projects related to data science, statistical learning, and deep learning could lead to new insights and methods for analyzing complex data in social, behavioral, and biomedical sciences.\n",
      "4. **All three professors**: Interdisciplinary collaborations on projects that combine econometrics, statistics, machine learning, and data science could lead to innovative solutions for complex problems in finance, economics, and social sciences.\n",
      "\n",
      "**Emerging Research Trends in Their Fields**\n",
      "\n",
      "1. **Econometrics and Statistics**: Increasing use of machine learning and artificial intelligence in econometrics, with a focus on causal inference and program evaluation.\n",
      "2. **Financial Econometrics**: Growing interest in high-frequency finance, algorithmic trading, and risk management, with a need for advanced statistical and computational methods.\n",
      "3. **Data Science and Machine Learning**: Rapid development of new techniques for deep learning, transfer learning, and explainable AI, with applications in computer vision, natural language processing, and recommender systems.\n",
      "4. **Computational Biology and Biostatistics**: Increasing use of machine learning and statistical techniques for analyzing large-scale biological data, with a focus on precision medicine and personalized healthcare.\n",
      "\n",
      "**Recommendations for Interdisciplinary Research Projects**\n",
      "\n",
      "1. **Developing machine learning methods for causal inference**: Collaborative project between Cattaneo, Fan, and Klusowski to develop and apply machine learning techniques for causal inference in social, behavioral, and biomedical sciences.\n",
      "2. **High-dimensional statistical analysis of financial networks**: Joint project between Fan and Klusowski to develop and apply high-dimensional statistical methods for analyzing financial networks and predicting systemic risk.\n",
      "3. **Data science for social good**: Interdisciplinary project between Cattaneo, Fan, and Klusowski to develop and apply data science techniques for addressing complex social problems, such as poverty, inequality, and climate change.\n",
      "4. **Integrating econometrics and machine learning for policy evaluation**: Collaborative project between Cattaneo and Fan to develop and apply integrated econometric and machine learning methods for evaluating the impact of policy interventions in social, behavioral, and biomedical sciences.\n",
      "\n",
      "These recommendations are designed to leverage the strengths of each professor's research focus, while promoting interdisciplinary collaboration and innovation. By working together, these researchers can develop new methods, models, and applications that address complex problems in finance, economics, and social sciences, and contribute to the advancement of knowledge in their fields.\n",
      "==================================================\n",
      "Token usage: CompletionUsage(completion_tokens=784, prompt_tokens=382, total_tokens=1166, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Test the same prompt with Llama via Novita\n",
    "print(\"Testing with Llama 3.3 70B via Novita...\")\n",
    "\n",
    "llama_response = novita_client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": formatted_prompt}\n",
    "    ],\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "llama_output = llama_response.choices[0].message.content\n",
    "llama_usage = llama_response.usage\n",
    "\n",
    "print(\"Llama Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(llama_output)\n",
    "print(\"=\" * 50)\n",
    "print(f\"Token usage: {llama_usage}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b874465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing outputs and generating improved prompt...\n",
      "Analysis and Improved Prompt:\n",
      "==================================================\n",
      "1. Key Differences in Quality, Depth, and Structure:\n",
      "\n",
      "The GPT-4 response provides a more detailed analysis of each professor's research focus, potential collaboration opportunities, emerging research trends, and recommendations for interdisciplinary research projects. It also provides a more comprehensive understanding of the professors' research interests and how they could potentially collaborate. The structure of the GPT-4 response is more coherent and flows better, with each section logically leading to the next.\n",
      "\n",
      "The Llama response, while also providing a detailed analysis, is somewhat more generic and lacks the depth of understanding displayed in the GPT-4 response. The structure is also less coherent, with the sections seeming more disjointed and less logically connected.\n",
      "\n",
      "2. Specific Aspects of the Llama Response That Could Be Improved:\n",
      "\n",
      "The Llama response could be improved by providing a more in-depth analysis of each professor's research focus and how their interests could potentially intersect. It could also benefit from a more logical and coherent structure, with each section leading naturally to the next. The recommendations for interdisciplinary research projects could be more specific and tailored to the professors' individual research interests.\n",
      "\n",
      "3. New, Optimized Prompt:\n",
      "\n",
      "You are an expert academic researcher. Please analyze the following professor profiles and provide:\n",
      "\n",
      "1. A detailed analysis of each professor's research focus, including the specific areas they specialize in and the methods they use.\n",
      "2. Potential collaboration opportunities between them, based on their individual research interests and areas of expertise.\n",
      "3. Emerging research trends in their fields, and how these trends could influence their future research.\n",
      "4. Specific recommendations for interdisciplinary research projects that could leverage the professors' combined expertise and address current challenges in their fields.\n",
      "\n",
      "Please ensure that your analysis is comprehensive, in-depth, and logically structured, with each section leading naturally to the next.\n",
      "\n",
      "4. Reasoning for the Prompt Improvements:\n",
      "\n",
      "The new prompt is more specific, asking for a detailed analysis of each professor's research focus and the methods they use. This should help Llama provide a more in-depth understanding of the professors' research interests. The prompt also asks for specific recommendations for interdisciplinary research projects, which should encourage Llama to provide more tailored and relevant suggestions. The request for a logically structured response should help improve the coherence and flow of Llama's output.\n",
      "==================================================\n",
      "Token usage: CompletionUsage(completion_tokens=456, prompt_tokens=1829, total_tokens=2285, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Feed both outputs to GPT for comparison and prompt improvement\n",
    "print(\"Analyzing outputs and generating improved prompt...\")\n",
    "\n",
    "comparison_prompt = f\"\"\"\n",
    "You are an expert in prompt engineering and AI model optimization. I have two responses to the same prompt from different AI models:\n",
    "\n",
    "ORIGINAL PROMPT:\n",
    "{formatted_prompt}\n",
    "\n",
    "OPENAI GPT-4 RESPONSE:\n",
    "{openai_output}\n",
    "\n",
    "LLAMA 3.3 70B RESPONSE:\n",
    "{llama_output}\n",
    "\n",
    "Please analyze these responses and:\n",
    "\n",
    "1. Identify the key differences in quality, depth, and structure between the two responses\n",
    "2. Determine what specific aspects of the Llama response could be improved\n",
    "3. Generate a new, optimized prompt that would help Llama produce output closer in quality to the GPT-4 response\n",
    "4. Explain your reasoning for the prompt improvements\n",
    "\n",
    "Focus on making the prompt more specific, providing better structure guidance, and addressing any weaknesses you observe in the Llama response.\n",
    "\"\"\"\n",
    "\n",
    "comparison_response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": comparison_prompt}\n",
    "    ],\n",
    "    max_tokens=3000,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "comparison_output = comparison_response.choices[0].message.content\n",
    "comparison_usage = comparison_response.usage\n",
    "\n",
    "print(\"Analysis and Improved Prompt:\")\n",
    "print(\"=\" * 50)\n",
    "print(comparison_output)\n",
    "print(\"=\" * 50)\n",
    "print(f\"Token usage: {comparison_usage}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec5aa3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting improved prompt from analysis...\n",
      "Extracted Improved Prompt:\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "\n",
      "Testing improved prompt with Llama...\n",
      "Improved Llama Response:\n",
      "==================================================\n",
      "It seems like you didn't ask a question or provide any text for me to respond to. Could you please provide more context or clarify what you would like to know? I'm here to help with any questions or topics you'd like to discuss.\n",
      "==================================================\n",
      "Token usage: CompletionUsage(completion_tokens=51, prompt_tokens=35, total_tokens=86, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Extract and test the improved prompt\n",
    "print(\"Extracting improved prompt from analysis...\")\n",
    "\n",
    "# Extract the improved prompt from the comparison output\n",
    "# This assumes the improved prompt is clearly marked in the response\n",
    "import re\n",
    "\n",
    "# Try to extract the improved prompt (this might need adjustment based on actual output format)\n",
    "improved_prompt_match = re.search(r'IMPROVED PROMPT:|NEW PROMPT:|OPTIMIZED PROMPT:(.*?)(?=\\n\\n|\\Z)', comparison_output, re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "if improved_prompt_match:\n",
    "    improved_prompt = improved_prompt_match.group(1).strip()\n",
    "else:\n",
    "    # If no clear marker, try to extract the last substantial block of text\n",
    "    lines = comparison_output.split('\\n')\n",
    "    improved_prompt = '\\n'.join(lines[-20:]).strip()  # Take last 20 lines as fallback\n",
    "\n",
    "print(\"Extracted Improved Prompt:\")\n",
    "print(\"=\" * 50)\n",
    "print(improved_prompt)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test the improved prompt with Llama\n",
    "print(\"\\nTesting improved prompt with Llama...\")\n",
    "\n",
    "improved_llama_response = novita_client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-3.3-70b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": improved_prompt}\n",
    "    ],\n",
    "    max_tokens=2000,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "improved_llama_output = improved_llama_response.choices[0].message.content\n",
    "improved_llama_usage = improved_llama_response.usage\n",
    "\n",
    "print(\"Improved Llama Response:\")\n",
    "print(\"=\" * 50)\n",
    "print(improved_llama_output)\n",
    "print(\"=\" * 50)\n",
    "print(f\"Token usage: {improved_llama_usage}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "236c473c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT OPTIMIZATION TEST SUMMARY\n",
      "============================================================\n",
      "\n",
      "1. ORIGINAL PROMPT:\n",
      "------------------------------\n",
      "\n",
      "You are an expert academic researcher. Please analyze the following professor profiles and provide:\n",
      "\n",
      "1. A summary of each professor's research focus\n",
      "2. Potential collaboration opportunities between t...\n",
      "\n",
      "2. OPENAI GPT-4 OUTPUT LENGTH: 3255 characters\n",
      "   Token usage: CompletionUsage(completion_tokens=541, prompt_tokens=356, total_tokens=897, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "\n",
      "3. LLAMA 3.3 70B OUTPUT LENGTH: 4460 characters\n",
      "   Token usage: CompletionUsage(completion_tokens=784, prompt_tokens=382, total_tokens=1166, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "\n",
      "4. ANALYSIS AND IMPROVED PROMPT LENGTH: 2630 characters\n",
      "   Token usage: CompletionUsage(completion_tokens=456, prompt_tokens=1829, total_tokens=2285, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))\n",
      "\n",
      "5. IMPROVED LLAMA OUTPUT LENGTH: 228 characters\n",
      "   Token usage: CompletionUsage(completion_tokens=51, prompt_tokens=35, total_tokens=86, completion_tokens_details=None, prompt_tokens_details=None)\n",
      "\n",
      "6. COMPARISON:\n",
      "------------------------------\n",
      "Original Llama vs Improved Llama:\n",
      "  Original: 4460 chars, 1166 tokens\n",
      "  Improved: 228 chars, 86 tokens\n",
      "\n",
      "Test completed! Check the outputs above to evaluate the effectiveness of the prompt optimization.\n"
     ]
    }
   ],
   "source": [
    "# Summary and Results\n",
    "print(\"PROMPT OPTIMIZATION TEST SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. ORIGINAL PROMPT:\")\n",
    "print(\"-\" * 30)\n",
    "print(formatted_prompt[:200] + \"...\" if len(formatted_prompt) > 200 else formatted_prompt)\n",
    "\n",
    "print(f\"\\n2. OPENAI GPT-4 OUTPUT LENGTH: {len(openai_output)} characters\")\n",
    "print(f\"   Token usage: {openai_usage}\")\n",
    "\n",
    "print(f\"\\n3. LLAMA 3.3 70B OUTPUT LENGTH: {len(llama_output)} characters\") \n",
    "print(f\"   Token usage: {llama_usage}\")\n",
    "\n",
    "print(f\"\\n4. ANALYSIS AND IMPROVED PROMPT LENGTH: {len(comparison_output)} characters\")\n",
    "print(f\"   Token usage: {comparison_usage}\")\n",
    "\n",
    "print(f\"\\n5. IMPROVED LLAMA OUTPUT LENGTH: {len(improved_llama_output)} characters\")\n",
    "print(f\"   Token usage: {improved_llama_usage}\")\n",
    "\n",
    "print(\"\\n6. COMPARISON:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Original Llama vs Improved Llama:\")\n",
    "print(f\"  Original: {len(llama_output)} chars, {llama_usage.total_tokens} tokens\")\n",
    "print(f\"  Improved: {len(improved_llama_output)} chars, {improved_llama_usage.total_tokens} tokens\")\n",
    "\n",
    "print(\"\\nTest completed! Check the outputs above to evaluate the effectiveness of the prompt optimization.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
